{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubinpeter/AIML/blob/main/Self_Supervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised learning\n",
        "\n",
        "In this tutorial, we will work with CIFAR10 dataset to see how much we can learn about the data without using labels."
      ],
      "metadata": {
        "id": "jyAJc8tlhGzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "y1zWUi2hh6Az"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GghrjSnGhFmI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm, metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the CIFAR10 dataset\n",
        "\n",
        "For this notebook, let us use only four classes from the CIFAR10 dataset"
      ],
      "metadata": {
        "id": "q7Cpa7jeiVYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# take only two classes: car, cat\n",
        "index = torch.tensor(trainset.targets)==1\n",
        "train_car_data = trainset.data[index]\n",
        "index = torch.tensor(trainset.targets)==3\n",
        "train_cat_data = trainset.data[index]\n",
        "\n",
        "# generate randomized data\n",
        "traindata = np.vstack((train_car_data, train_cat_data))\n",
        "trainlabel = np.zeros(len(traindata))\n",
        "trainlabel[5000:10000] = 1\n",
        "index = np.random.permutation(len(traindata))\n",
        "traindata = traindata[index, :,:,:]\n",
        "trainlabel = trainlabel[index]\n",
        "\n",
        "# create the test set\n",
        "\n",
        "# take only two classes: car, cat\n",
        "index = torch.tensor(testset.targets)==1\n",
        "test_car_data = testset.data[index]\n",
        "index = torch.tensor(testset.targets)==3\n",
        "test_cat_data = testset.data[index]\n",
        "\n",
        "# generate randomized data\n",
        "testdata = np.vstack((test_car_data, test_cat_data))\n",
        "testlabel = np.zeros(len(testdata))\n",
        "testlabel[1000:2000] = 1\n",
        "index = np.random.permutation(len(testdata))\n",
        "testdata = testdata[index, :,:,:]\n",
        "testlabel = testlabel[index]"
      ],
      "metadata": {
        "id": "Qtld4mfOiMsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us visualize some images from the classes"
      ],
      "metadata": {
        "id": "gw2kSWOVi1ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# car images\n",
        "randind = torch.randint(len(train_car_data), size=(5,1))\n",
        "fig, ax = plt.subplots(1, 5)\n",
        "for ii in range(5):\n",
        "  ax[ii].imshow(train_car_data[randind[ii]])\n",
        "\n",
        "# cat images\n",
        "randind = torch.randint(len(train_cat_data), size=(5,1))\n",
        "fig, ax = plt.subplots(1, 5)\n",
        "for ii in range(5):\n",
        "  ax[ii].imshow(train_cat_data[randind[ii]])\n",
        "\n",
        "# random images\n",
        "randind = torch.randint(len(traindata), size=(5,1))\n",
        "fig, ax = plt.subplots(1, 5)\n",
        "for ii in range(5):\n",
        "  ax[ii].imshow(traindata[randind[ii]])"
      ],
      "metadata": {
        "id": "Zgus0bbwimkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize a PCA-reduced data"
      ],
      "metadata": {
        "id": "nVN8WxM3yiiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(traindata, (len(traindata), -1))\n",
        "pca = PCA(n_components=2).fit(X)\n",
        "car_pca = pca.transform(np.reshape(train_car_data, (len(train_car_data), -1)))\n",
        "plt.scatter(car_pca[:,0], car_pca[:,1])\n",
        "cat_pca = pca.transform(np.reshape(train_cat_data, (len(train_cat_data), -1)))\n",
        "plt.scatter(cat_pca[:,0], cat_pca[:,1])\n",
        "plt.legend(['car','cat'])"
      ],
      "metadata": {
        "id": "u94vAjlyyhn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the features of each overlap and there is no clear separation. This means that the input images by themselves are not good features.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d8zM-PF7xLN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means clustering directly on the image data"
      ],
      "metadata": {
        "id": "dNsfPkR9vtBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see the mix of data in each cluster"
      ],
      "metadata": {
        "id": "9JlhoHQV2aPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_kmeans(kmeans):\n",
        "  fig, ax = plt.subplots(1,2)\n",
        "\n",
        "  # cluster = 0\n",
        "  l_cluster = trainlabel[kmeans.labels_==0]\n",
        "  class0 = np.sum(l_cluster==0)\n",
        "  class1 = np.sum(l_cluster==1)\n",
        "  ax[0].bar(['Car','Cat'],[class0,class1])\n",
        "\n",
        "  # cluster = 1\n",
        "  l_cluster = trainlabel[kmeans.labels_==1]\n",
        "  class0 = np.sum(l_cluster==0)\n",
        "  class1 = np.sum(l_cluster==1)\n",
        "  ax[1].bar(['Car','Cat'],[class0,class1])\n"
      ],
      "metadata": {
        "id": "4m9LZvQNy9OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(traindata, (len(traindata),-1))\n",
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "CbQ1MZqL63ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_kmeans(kmeans)"
      ],
      "metadata": {
        "id": "DQVy-bV9699d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means clustering on PCA-reduced data\n",
        "\n",
        "Instead of applying K-means directly on the image data, let us first reduce the dimensionality of the image data from 32x32x3 to 10 and then apply kmeans"
      ],
      "metadata": {
        "id": "jw1TMPow6lOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.reshape(traindata, (len(traindata), -1))\n",
        "pca = PCA(n_components=10).fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X_pca)\n",
        "visualize_kmeans(kmeans)"
      ],
      "metadata": {
        "id": "ZXKHGBCE7KZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM on PCA-reduced data\n",
        "\n",
        "We are using SVM instead of a deep network because we want a less powerful calssifier for this experiment.\n",
        "\n",
        "Here. we will reduce the dimensions of our image data into 10 features and classify the features using SVM into car and cat"
      ],
      "metadata": {
        "id": "d1_urWKaRPhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce the dimensions to 10 using PCA\n",
        "train = np.reshape(traindata, (len(traindata), -1))\n",
        "pca = PCA(n_components=10).fit(X)\n",
        "train_pca = pca.transform(X)\n",
        "test = np.reshape(testdata, (len(testdata), -1))\n",
        "test_pca = pca.transform(X)\n",
        "\n",
        "# classify using SVM and see the accuracy\n",
        "# create a model using SVM\n",
        "clf = svm.LinearSVC()\n",
        "clf.fit(train, trainlabel)\n",
        "test_predict = clf.predict(test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(test_predict, testlabel))"
      ],
      "metadata": {
        "id": "qEASO4F5RgwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding images using an auxiliary task\n",
        "\n",
        "Let us use the task of predicting the orientation of an image to learn better features than PCA. We will have four classes: up, down, left and right. Here are what the images look like:"
      ],
      "metadata": {
        "id": "Kc9VKqkx6VTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = traindata[1001,:,:,:]\n",
        "fig, ax = plt.subplots(1,5)\n",
        "ax[0].imshow(img)\n",
        "ax[0].set_title('Original')\n",
        "img = np.rot90(img)\n",
        "ax[1].imshow(img)\n",
        "ax[1].set_title('Left')\n",
        "img = np.rot90(img)\n",
        "ax[2].imshow(img)\n",
        "ax[2].set_title('Down')\n",
        "img = np.rot90(img)\n",
        "ax[3].imshow(img)\n",
        "ax[3].set_title('Right')\n",
        "img = np.rot90(img)\n",
        "ax[4].imshow(img)\n",
        "ax[4].set_title('Up')"
      ],
      "metadata": {
        "id": "3-50fF8WkVIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataset for this task"
      ],
      "metadata": {
        "id": "b4Cdtdrd864Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotationData(torch.utils.data.Dataset):\n",
        "  def __init__(self, data):\n",
        "    super(RotationData, self).__init__()\n",
        "    self.data=data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = torch.tensor(self.data[index]/255, dtype=torch.float32);\n",
        "    # get a random class\n",
        "    label = torch.randint(4,[1,1]).item()\n",
        "    for ii in range(label):\n",
        "      img = torch.rot90(img)\n",
        "    img = img.permute(2,0,1) # make channels first\n",
        "    return img, label\n",
        "\n",
        "traindataset = RotationData(traindata)\n",
        "dataloader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testdataset = RotationData(testdata)"
      ],
      "metadata": {
        "id": "HcWK0Gkzm5Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a model for our task. Notice that our final layer has output of size 4, for the four classes. The penultimate layer has size 10, because we want to obtain a size of 10."
      ],
      "metadata": {
        "id": "r71WjsUP_L_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        # output layer 4 classes for each orientation\n",
        "        self.fc3 = nn.Linear(10,4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    # so that we can get only the features\n",
        "    def get_features(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      # flatten all dimensions except batch\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      return x"
      ],
      "metadata": {
        "id": "1Uj7TZyA42mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training function"
      ],
      "metadata": {
        "id": "1Eo_uxR7BKCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "-8abaVQH5Ahz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us train the model"
      ],
      "metadata": {
        "id": "jBHnco9cBczS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start with a fresh model\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD optimizer with momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "\n"
      ],
      "metadata": {
        "id": "97Cdtwos-ErJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 40  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, dataloader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "1SS9ETUEBq9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ar0ieZnyB3B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see how good these features are. First, we need to extract the features for our data.\n",
        "\n"
      ],
      "metadata": {
        "id": "e_mJ2o7VEUp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainfeatures = torch.zeros((len(traindataset), 10)) # 10 is the number of features\n",
        "dataloader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "for i, (images, labels) in enumerate(dataloader):\n",
        "  trainfeatures[i*batch_size: (i+1)*batch_size, :] = model.get_features(images)\n",
        "\n",
        "trainfeatures = trainfeatures.data\n",
        "\n",
        "\n",
        "testfeatures = torch.zeros((len(testdataset), 10)) # 10 is the number of features\n",
        "dataloader = dataloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "for i, (images, labels) in enumerate(dataloader):\n",
        "  testfeatures[i*batch_size: (i+1)*batch_size, :] = model.get_features(images)\n",
        "\n",
        "testfeatures = testfeatures.data"
      ],
      "metadata": {
        "id": "Sy059pCoDgH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run SVM and check the accuracy"
      ],
      "metadata": {
        "id": "LvV5Ljq6EThU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.LinearSVC()\n",
        "clf.fit(trainfeatures, trainlabel)\n",
        "test_predict = clf.predict(testfeatures)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(test_predict, testlabel))"
      ],
      "metadata": {
        "id": "fmnY7ceMXXi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KMeans on the features we found"
      ],
      "metadata": {
        "id": "cxQRKtJSc2hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(trainfeatures)\n",
        "visualize_kmeans(kmeans)"
      ],
      "metadata": {
        "id": "mYKyvTBUGXDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}